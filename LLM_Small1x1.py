# -*- coding: utf-8 -*-
"""LLM-Playground.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eLTdG2nEmsl7CIeFC15j1DQ7EwD1SaYS
"""

import time
startTime = time.time()

!pip install lorem
!pip install tiktoken

from keras.datasets import mnist
from keras.utils import to_categorical

import colorsys
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import numpy as np
import random

import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch import nn
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Subset

import lorem
import tiktoken
import os
import json
import requests
import tensorflow as tf
from tqdm import tqdm

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(device)

import plotly.io as pio
pio.renderers.default = 'colab'

def time_since_start(start_time):
    current_time = time.time()
    elapsed_time = current_time - start_time

    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = int(elapsed_time % 60)
    milliseconds = int((elapsed_time % 1) * 1000)

    return f"{hours}h {minutes}m {seconds}s {milliseconds}ms"

"""#Initialization"""

small1x1 = []

def createUniqueCalculation(createdCalculations, xMin = 0, xMax = 9, yMin = 0, yMax = 9):
    x = random.randint(xMin, xMax)
    y = random.randint(yMin, yMax)
    while (x, y) in createdCalculations:
        x = random.randint(xMin, xMax)
        y = random.randint(yMin, yMax)
    createdCalculations.append((x, y))
    return x, y

def createSmall1x1():
    global small1x1
    createdCalculations = []
    small1x1 = []

    for trainExample in range(100):
        x, y = createUniqueCalculation(createdCalculations, 0, 9, 0, 9)
        small1x1.append((x, y, x*y, lorem.sentence() + f"{x}*{y}={x*y}"))

GPT_CONFIG_124M = {
    "vocab_size": 50257,   # Vocabulary size
    "context_length": 256, # Shortened context length (orig: 1024)
    "emb_dim": 768,        # Embedding dimension
    "n_heads": layerAmountChoice.value,         # Number of attention heads
    "n_layers": layerAmountChoice.value,        # Number of layers
    "drop_rate": 0.1,      # Dropout rate
    "qkv_bias": False      # Query-key-value bias
}

settings = {"learning_rate": learningRateChoice.value, "weight_decay": 0.1, "batch_size": 1, "num_epochs": epochsChoice.value}

LLM_Layers = [[('Embedding', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["vocab_size"]),
 ('Embedding', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"])],
 #('Dropout', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["drop_rate"])],
  [('LayerNorm', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 ('Linear', GPT_CONFIG_124M["vocab_size"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["vocab_size"])]]

TransformerBlockLayer = [('LayerNorm', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 ('Linear', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 ('Linear', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 ('Linear', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 #('Dropout', 2, GPT_CONFIG_124M["drop_rate"]),
 ('Linear', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"]),
 ('MultiHeadAttention', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["emb_dim"])]
 #('Dropout', GPT_CONFIG_124M["emb_dim"], GPT_CONFIG_124M["drop_rate"])]

"""#Data Initialization"""

class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        # Tokenize the entire text
        token_ids = tokenizer.encode(txt)

        # Use a sliding window to chunk the book into overlapping sequences of max_length
        for i in range(0, len(token_ids) - max_length, stride):
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.target_ids[idx]

def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True, num_workers=0):
    # Initialize the tokenizer
    tokenizer = tiktoken.get_encoding("gpt2")

    # Create dataset
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)

    # Create dataloader
    dataloader = DataLoader(
        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=0)

    return dataloader

def createLLMLoaders():
    actualSamples = [sample[-1] for sample in small1x1]
    samples = "\n".join(actualSamples[:train_samples])

    train_loader = create_dataloader_v1(
        samples,
        batch_size=settings["batch_size"],
        max_length=1,
        stride=1,
        drop_last=False,
        shuffle=False,
        num_workers=0
    )

    samples = "\n".join(actualSamples[train_samples:train_samples+test_samples])
    val_loader = create_dataloader_v1(
        samples,
        batch_size=settings["batch_size"],
        max_length=1,
        stride=1,
        drop_last=False,
        shuffle=False,
        num_workers=0
    )

    samples = "\n".join(actualSamples[train_samples+test_samples:train_samples+test_samples+eval_samples])
    eval_loader = create_dataloader_v1(
        samples,
        batch_size=settings["batch_size"],
        max_length=1,
        stride=1,
        drop_last=False,
        shuffle=False,
        num_workers=0
    )

    return train_loader, val_loader, eval_loader

"""# Setup Customizable Network"""

class MultiHeadAttention(nn.Module):
    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):
        super().__init__()
        assert d_out % num_heads == 0, "d_out must be divisible by n_heads"

        self.d_out = d_out
        self.num_heads = num_heads
        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim

        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs
        self.dropout = nn.Dropout(dropout)
        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))

    def forward(self, x):
        b, num_tokens, d_in = x.shape

        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)
        queries = self.W_query(x)
        values = self.W_value(x)

        # We implicitly split the matrix by adding a `num_heads` dimension
        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)
        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)
        values = values.view(b, num_tokens, self.num_heads, self.head_dim)
        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)

        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)
        keys = keys.transpose(1, 2)
        queries = queries.transpose(1, 2)
        values = values.transpose(1, 2)

        # Compute scaled dot-product attention (aka self-attention) with a causal mask
        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head

        # Original mask truncated to the number of tokens and converted to boolean
        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]

        # Use the mask to fill attention scores
        attn_scores.masked_fill_(mask_bool, -torch.inf)

        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # Shape: (b, num_tokens, num_heads, head_dim)
        context_vec = (attn_weights @ values).transpose(1, 2)

        # Combine heads, where self.d_out = self.num_heads * self.head_dim
        context_vec = context_vec.reshape(b, num_tokens, self.d_out)
        context_vec = self.out_proj(context_vec)  # optional projection

        return context_vec

class LayerNorm(nn.Module):
    def __init__(self, emb_dim):
        super().__init__()
        self.eps = 1e-5
        self.scale = nn.Parameter(torch.ones(emb_dim))
        self.shift = nn.Parameter(torch.zeros(emb_dim))

    def forward(self, x):
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        norm_x = (x - mean) / torch.sqrt(var + self.eps)
        return self.scale * norm_x + self.shift

class GELU(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / torch.pi)) *
            (x + 0.044715 * torch.pow(x, 3))
        ))

class FeedForward(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
            GELU(),
            nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
        )

    def forward(self, x):
        return self.layers(x)

class TransformerBlock(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.att = MultiHeadAttention(
            d_in=cfg["emb_dim"],
            d_out=cfg["emb_dim"],
            context_length=cfg["context_length"],
            num_heads=cfg["n_heads"],
            dropout=cfg["drop_rate"],
            qkv_bias=cfg["qkv_bias"])
        self.ff = FeedForward(cfg)
        self.norm1 = LayerNorm(cfg["emb_dim"])
        self.norm2 = LayerNorm(cfg["emb_dim"])
        self.drop_resid = nn.Dropout(cfg["drop_rate"])

    def forward(self, x):
        # Shortcut connection for attention block
        shortcut = x
        x = self.norm1(x)
        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        # Shortcut connection for feed-forward block
        shortcut = x
        x = self.norm2(x)
        x = self.ff(x)
        x = self.drop_resid(x)
        x = x + shortcut  # Add the original input back

        return x

class GPTModel(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])

        self.trf_blocks = nn.Sequential(
            *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])])

        self.final_norm = LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        batch_size, seq_len = in_idx.shape
        in_idx.to(device)
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))
        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]
        x = self.drop_emb(x)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits

# All necessary variables for training
input_size = len(train_data[0][0])
output_size = len(train_data[0][1])

model = GPTModel(GPT_CONFIG_124M)
layers = hidden_sizes
model.to(device)

if(loss_function == "MSE"):
    criterion_class = nn.MSELoss()  # For regression
elif(loss_function == "Cross-Entropy"):
    criterion_class = nn.CrossEntropyLoss()  # For multi-class classification

if(optimizer == "Adam"):
    chosen_optimizer = optim.Adam(model.parameters(), lr=learning_rate)
elif(optimizer == "SGD"):
    chosen_optimizer = optim.SGD(model.parameters(), lr=learning_rate)

"""# Training Customizable RENN"""

def text_to_token_ids(text, tokenizer):
    encoded = tokenizer.encode(text)
    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension
    return encoded_tensor


def token_ids_to_text(token_ids, tokenizer):
    flat = token_ids.squeeze(0)  # remove batch dimension
    return tokenizer.decode(flat.tolist())

def calc_loss_batch(input_batch, target_batch, model, device):
    input_batch, target_batch = input_batch.to(device), target_batch.to(device)
    logits = model(input_batch)
    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())
    return loss


def calc_loss_loader(data_loader, model, device, num_batches=None):
    total_loss = 0.
    if num_batches is None:
        num_batches = len(data_loader)
    else:
        num_batches = min(num_batches, len(data_loader))
        print("Dataloader: ", num_batches, ", ", len(data_loader))
    for i, (input_batch, target_batch) in enumerate(data_loader):
        if i < num_batches:
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            total_loss += loss.item()
        else:
            break
    return total_loss / num_batches

def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,
                       eval_freq, eval_iter, start_context, tokenizer):
    # Initialize lists to track losses and tokens seen
    train_losses, val_losses, track_tokens_seen = [], [], []
    tokens_seen = 0
    global_step = -1

    # Main training loop
    for epoch in range(num_epochs):
        model.train()  # Set model to training mode

        for input_batch, target_batch in train_loader:
            optimizer.zero_grad()  # Reset loss gradients from previous epoch
            loss = calc_loss_batch(input_batch, target_batch, model, device)
            loss.backward()  # Calculate loss gradients
            optimizer.step()  # Update model weights using loss gradients
            tokens_seen += input_batch.numel()
            global_step += 1

            # Optional evaluation step
            #if global_step % eval_freq == 0:
        model.eval()
        with torch.no_grad():
            train_loss, val_loss = evaluate_model(
                model, train_loader, val_loader, device, eval_iter)
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            track_tokens_seen.append(tokens_seen)
        #print(f"Ep {epoch+1} (Step {global_step:06d}): "f"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}")
        print(f"Ep {epoch+1}: "f"Train loss {train_loss}, Val loss {val_loss}")


    return train_losses, val_losses, track_tokens_seen

def evaluate_model(model, train_loader, val_loader, device, eval_iter):
    model.eval()
    with torch.no_grad():
        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)
        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)
    model.train()
    return train_loss, val_loss

def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):
    fig, ax1 = plt.subplots()

    # Plot training and validation loss against epochs
    ax1.plot(epochs_seen, train_losses, label="Training loss")
    ax1.plot(epochs_seen, val_losses, linestyle="-.", label="Validation loss")
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Loss")
    ax1.legend(loc="upper right")

    # Create a second x-axis for tokens seen
    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis
    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks
    ax2.set_xlabel("Tokens seen")

    fig.tight_layout()  # Adjust layout to make room
    # plt.show()

def main(model, settings):
    optimizer = torch.optim.AdamW(
        model.parameters(), lr=settings["learning_rate"], weight_decay=settings["weight_decay"]
    )

    tokenizer = tiktoken.get_encoding("gpt2")

    train_loader, val_loader, eval_loader = createLLMLoaders()

    train_losses, val_losses, tokens_seen = train_model_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=settings["num_epochs"], eval_freq=5, eval_iter=1,
        start_context="=", tokenizer=tokenizer
    )

    return train_losses, val_losses, tokens_seen, train_loader, eval_loader, model

# Train the model
trainTime = time.time()
_, _, _, train_dataloader, eval_dataloader, model = main(model, settings)
tokenizer = tiktoken.get_encoding("gpt2")
print(f"Time passed since start of training: {time_since_start(trainTime)}")

"""# Sourcecheck Customisable RENN"""

layerSizes = [size[1] for size in hidden_sizes[:]]
totalLayers = len(layers)
activationsBySources = np.full((train_samples, totalLayers, np.max(layerSizes)), 100000)
activationsByLayers = np.full((totalLayers, np.max(layerSizes), train_samples), 100000)
print(activationsBySources.shape)
print(activationsByLayers.shape)

#Variables for usage within the hook
layer = 0
source = 0
dictionaryForSourceLayerNeuron = activationsBySources
dictionaryForLayerNeuronSource = activationsByLayers
print(layers)

# Forward hook
def forward_hook(module, input, output):
    global layer
    global source
    global dictionaryForSourceLayerNeuron
    global dictionaryForLayerNeuronSource
    global hidden_sizes
    global datasetChoice

    if not (isinstance(module, nn.Sequential) or isinstance(module, FeedForward) or isinstance(module, TransformerBlock) or isinstance(module, nn.Dropout)):
        actualLayer = layer
        layerNeurons = layers[actualLayer][1]
        if(source >= dictionaryForSourceLayerNeuron.shape[0]):
            return

        activation_type = type(getActivation(hidden_sizes, actualLayer))
        layer_type = type(getLayer(hidden_sizes, actualLayer))
        relevantOutput = output[0].cpu().numpy()

        #print(layer, layers[layer], relevantOutput.shape)

        #Use for array structure like: [source, layer, neuron]
        if(len(relevantOutput.shape) > 1):
            if(relevantOutput.shape[1] != layerNeurons):
                layerNeurons = relevantOutput.shape[1]
                #layers[actualLayer] = (layers[actualLayer][0], relevantOutput.shape[1], layers[layer][2:])
        dictionaryForSourceLayerNeuron[source][layer,:layerNeurons] = relevantOutput

        #Use for array structure like: [layer, neuron, source]
        # if(datasetChoice.value == "Small 1x1"):
        #     if()
        #     print(actualLayer, relevantOutput.shape, len(relevantOutput.shape))
        #     for neuronNumber in range(layers[actualLayer][1]):
        #         if neuronNumber < layerNeurons:
        #             dictionaryForLayerNeuronSource[actualLayer][neuronNumber][source] = relevantOutput[0][neuronNumber]
        #         else:
        #             break
        # else:
        output = relevantOutput if len(relevantOutput.shape) == 1 else relevantOutput[0]
        for neuronNumber, neuron in enumerate(output):
            if neuronNumber < layerNeurons:
                dictionaryForLayerNeuronSource[actualLayer][neuronNumber][source] = neuron
            else:
                break

        if((layer == (len(layers)*2)-1 and not datasetChoice.value == "Small 1x1") or (layer == (len(layers))-1 and datasetChoice.value == "Small 1x1")):
            layer = 0
        else:
            layer += 1
#-------------------------------------------------------------------------------

def attachHooks(hookLoader, model):
    global source
    layer = 0
    source = 0

    hooks = []  # Store the handles for each hook
    outputs = np.array([])

    for name, module in model.named_modules():
      if not isinstance(module, type(model)):
          hook = module.register_forward_hook(forward_hook)
          hooks.append(hook)

    with torch.no_grad():
      # Forward Pass
      convertTime = time.time()
      for source, (inputs, labels) in enumerate(hookLoader):
        inputs = inputs.to(device)
        _ = model(inputs)

      print(f"Time passed since conversion: {time_since_start(convertTime)}")

    # Remove hooks after use
    for hook in hooks:
        hook.remove()

attachHooks(train_dataloader, model)
activationsBySources = dictionaryForSourceLayerNeuron
activationsByLayers = dictionaryForLayerNeuronSource

activationsBySources[0][0]

"""# Visualization Customisable RENN"""

def generate(model, idx, max_new_tokens, context_size, temperature, top_k=None):

    # For-loop is the same as before: Get logits, and only focus on last time step
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond.to(device))
        logits = logits[:, -1, :]

        # New: Filter logits with top_k sampling
        if top_k is not None:
            # Keep only top_k values
            top_logits, _ = torch.topk(logits, top_k)
            min_val = top_logits[:, -1]
            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)

        # New: Apply temperature scaling
        if temperature > 0.0:
            logits = logits / temperature

            # Apply softmax to get probabilities
            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)

            # Sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)

        # Otherwise same as before: get idx of the vocab entry with the highest logits value
        else:
            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)

        # Same as before: append sampled index to the running sequence
        idx = torch.cat((idx.to(device), idx_next.to(device)), dim=1)  # (batch_size, num_tokens+1)

    return idx

def getLLMPrediction(sample):
    x, y, solution, _ = sample
    index = text_to_token_ids(f"{x}*{y}=", tokenizer)
    maxNewTokens = len(str(x*y))
    contextSize = len(str(f"{x}*{y}="))
    token_ids = generate(model=model, idx=index, max_new_tokens=maxNewTokens,
    context_size=contextSize, top_k=1, temperature=1.0)
    prediction = token_ids_to_text(token_ids, tokenizer)
    return x, y, solution, prediction

#Make sure to set new dictionarys for the hooks to fill - they are global!
dictionaryForSourceLayerNeuron = np.full((eval_samples, totalLayers, np.max(layerSizes)), 100000)
dictionaryForLayerNeuronSource = np.full((totalLayers, np.max(layerSizes), eval_samples), 100000)

with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    attachHooks(eval_dataloader, model)

for sampleNumber in range(eval_samples):
    #TODO: Save calculations to file and hook in evaluation mode onto the model!
    sources, outputs, layerNumbersToCheck = identifyClosestSources(activationsByLayers, dictionaryForSourceLayerNeuron[sampleNumber])
    #print(activationsByLayers, dictionaryForSourceLayerNeuron[sampleNumber])
    mostUsedSources = getMostUsedSources(sources, "")
    x, y, solution, prediction = getLLMPrediction(small1x1[train_samples+test_samples+sampleNumber])
    print(prediction, f" -> Difference = {(solution) - (int(prediction) if prediction.isdigit() else 0)}")
    print([(sourceNumber, count, small1x1[sourceNumber][3]) for sourceNumber, count in mostUsedSources])

print(f"Time passed since start: {time_since_start(startTime)}")
